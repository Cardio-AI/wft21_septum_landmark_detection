wft21_septum_landmark_detection
==============================

A short description of the project.

Project Organization
------------

    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ Makefile           <- Makefile with commands like 'make environment' or 'make requirement'
    â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
    â”œâ”€â”€ data
    â”‚Â Â  â”œâ”€â”€ metadata       <- Excel and csv files with additional metadata
    â”‚Â Â  â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
    â”‚Â Â  â”œâ”€â”€ predicted      <- Model predictions, will be used for the evaluations
    â”‚Â Â  â””â”€â”€ raw            <- The original, immutable data dump.
    â”‚
    â”‚
    â”œâ”€â”€ notebooks          <- Jupyter notebooks. 
    â”‚Â Â  â”œâ”€â”€ Dataset        <- call the dataset helper functions, to analyze and slice the Dataset
    â”‚Â Â  â”œâ”€â”€ Evaluate       <- See further below, reference to google-colab
    â”‚Â Â  â”œâ”€â”€ Predict        <- Generate predictions for each fold
    â”‚Â Â  â”œâ”€â”€ Train          <- Train a new model
    â”‚
    â”œâ”€â”€ exp            <- Generated analysis as HTML, PDF, LaTeX, etc. Saved locally, if axes is needed , let us know
    â”‚Â Â  â”œâ”€â”€ configs        <- Experiment config files as json
    â”‚Â Â  â”œâ”€â”€ figures        <- Generated graphics and figures to be used in reporting
    â”‚Â Â  â”œâ”€â”€ history        <- Tensorboard trainings history files
    â”‚Â Â  â”œâ”€â”€ models         <- Trained and serialized models, model predictions, or model summaries
    â”‚Â Â  â”œâ”€â”€ models.png     <- Model summary as picture
    â”‚Â Â  â””â”€â”€ tensorboard_logs  <- Generated graphics and figures to be used in reporting
    â”‚
    â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    â”‚                         generated with `pip freeze > requirements.txt`
    â”‚
    â”œâ”€â”€ setup.py           <- Makes project pip installable (pip install -e .) so src can be imported
    â”œâ”€â”€ src                <- Helper functions that will be used by the notebooks.
     Â Â  â”œâ”€â”€ data           <- create, preprocess and extract the nrrd files
     Â Â  â”œâ”€â”€ models         <- Modelzoo, Modelutils and Tensorflow layers
     Â Â  â”œâ”€â”€ utils          <- Metrics, callbacks, io-utils, notebook imports
     Â Â  â””â”€â”€ visualization  <- Plots for the data, generator or evaluations

Praktikum Data-Science in der Medizin
------------ 
We used Google-Colab for our Evaluation Script. In the `notebooks` folder you will find the main notebooks we used, in `src` you will find functions from the Code Base.
A huge thanks to Sven KÃ¶hler for his kind supervision, help and code. âœ¨ ðŸŒŸ ðŸ’«
### Evaluation-Script for Interobserver Variability and Variability between Predictions and Ground-Truth
#### Link to Google Colab: https://colab.research.google.com/drive/1-wiDHzgGkPV13ZMaZLkvk5IYZSi6U5qm?usp=sharing
------------
### Link to Google Colab:


Setup native with OSX or Ubuntu
------------
### Preconditions: 
- Python 3.6 locally installed 
(e.g.:  <a target="_blank" href="https://www.anaconda.com/download/#macos">Anaconda</a>)
- Installed nvidia drivers, cuda and cudnn 
(e.g.:  <a target="_blank" href="https://www.tensorflow.org/install/gpu">Tensorflow</a>)

### Local setup
Clone repository
```
git clone %repo-name%
cd %repo-name%
```
Create a conda environment from environment.yaml (environment name will be foo)
```
conda env create --file environment.yaml
```

Activate environment
```
conda activate foo
```
Install a helper to automatically change the working directory to the project root directory
```
pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot
```
Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab
```
python -m ipykernel install --user --name foo --display-name "foo kernel"
```
